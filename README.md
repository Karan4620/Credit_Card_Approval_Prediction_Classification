# ğŸ’³ Credit Card Approval Prediction System

## ğŸ“Œ Project Overview
This project builds an **end-to-end machine learning pipeline** to predict **credit card approval decisions** based on applicant demographic and financial attributes. The solution covers the full ML lifecycle â€” from data ingestion and preprocessing to model training, evaluation, and deployment readiness.

The goal is to simulate a **real-world credit risk assessment system**, emphasizing clean architecture, reproducibility, and scalability.

---

## ğŸ¯ Problem Statement
Financial institutions must efficiently assess whether a credit card application should be approved or rejected while minimizing risk and bias.

Given applicant information such as income, employment status, education, and financial indicators, this project aims to:
- Predict **approval status (Yes / No)**
- Handle **imbalanced class distribution**
- Compare multiple classification models
- Deliver interpretable performance metrics

---

## ğŸ“Š Dataset Description
- **Source:** Publicly available credit card application dataset  
- **Target Variable:** Credit Card Approval (Approved / Not Approved)  
- **Features include:**
  - Demographic attributes (gender, age, education, marital status)
  - Financial indicators (income type, housing status)
  - Employment and credit-related variables
- **Challenges addressed:**
  - Mixed data types (categorical + numerical)
  - Missing values
  - Class imbalance

---

## ğŸ§° Tools & Technologies
- **Language:** Python  
- **Data Analysis:** Pandas, NumPy  
- **Visualization:** Matplotlib, Seaborn  
- **Machine Learning:** Scikit-learn, Imbalanced-learn (SMOTE)  
- **Model Persistence:** Joblib  
- **Deployment Ready:** Streamlit, AWS S3  

---

## ğŸ—ï¸ Project Structure
```
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ Credit_Card_Code_rewritten.ipynb
â”œâ”€â”€ saved_models/
â”‚   â””â”€â”€ <model_name>/
â”œâ”€â”€ saved_models_final/
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
```

---

## ğŸ”„ Workflow & Methodology

### 1ï¸âƒ£ Data Loading & Preparation
- Combined training and test datasets
- Randomized records to prevent ordering bias
- Separated features and target variable

### 2ï¸âƒ£ Data Preprocessing
- Numerical features scaled using **MinMaxScaler**
- Categorical features encoded using **OneHotEncoder** and **OrdinalEncoder**
- Implemented preprocessing via **Scikit-learn Pipelines**
- Ensured reproducibility and clean transformations

### 3ï¸âƒ£ Handling Class Imbalance
- Applied **SMOTE (Synthetic Minority Oversampling Technique)**
- Prevented model bias toward majority class
- Integrated SMOTE directly into the training pipeline

### 4ï¸âƒ£ Model Training
Multiple classifiers were trained and evaluated, including:
- Logistic Regression
- Decision Tree
- Random Forest
- Gradient Boosting
- Support Vector Machine (SVM)

Each model was trained using **cross-validation** to ensure robust performance estimates.

### 5ï¸âƒ£ Model Evaluation
Models were evaluated using:
- Accuracy
- Precision
- Recall
- F1-Score
- ROC-AUC Curve
- Confusion Matrix

Visual diagnostics were generated for each model to enable **comparative analysis**.

### 6ï¸âƒ£ Model Persistence
- Trained models and predictions were serialized using **Joblib**
- Saved to disk with versioned directories
- Supports reproducibility and future inference

---

## ğŸ“ˆ Key Results & Insights
- Ensemble-based models (Random Forest, Gradient Boosting) consistently outperformed linear models
- SMOTE significantly improved recall for the minority class
- Pipeline-based preprocessing ensured zero data leakage
- SVM showed strong performance but required longer training time

---

## ğŸš€ Deployment Readiness
The project is structured for deployment with:
- Serialized models compatible with **Streamlit**
- Support for loading models from **AWS S3**
- Modular functions that separate training, evaluation, and inference logic

This makes the system suitable for:
- Web apps
- Internal decision-support tools
- API-based inference services

---

## â–¶ï¸ How to Run the Project

1. Clone the repository
```bash
git clone <repo-url>
cd credit-card-approval-prediction
```

2. Install dependencies
```bash
pip install -r requirements.txt
```

3. Run the notebook
```bash
jupyter notebook
```

4. (Optional) Launch Streamlit app
```bash
streamlit run app.py
```

---

## ğŸ”® Future Improvements
- Hyperparameter optimization using GridSearchCV / Optuna
- Fairness and bias analysis
- SHAP-based explainability
- Model monitoring and drift detection
- Full REST API deployment

---

## ğŸ‘¤ Author
**Karan Sangukrishnan**  
MS in Business Analytics  

---

## â­ Why This Project Stands Out
- End-to-end ML lifecycle implementation  
- Production-style pipelines and structure  
- Real-world challenges: imbalance, preprocessing, evaluation  
- Deployment-aware design  
